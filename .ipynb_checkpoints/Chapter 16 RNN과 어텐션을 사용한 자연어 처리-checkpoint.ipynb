{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고](https://github.com/rickiepark/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 챗봇(chatbot) : 대화의 상대방이 자신을 사람이라고 생각하도록 속일 수 있는 기계.\n",
    "- 문자 단위 RNN(character RNN) : 문장에서 다음 글자를 예측하도록하는 훈련\n",
    "- **상태가 없는 RNN**(stateless RNN)을 사용하고 **상태가 있는 RNN**(stateful RNN)을 구축한다.\n",
    "- 텐서플로 애드온(Addon) 프로젝트에서 제공하는 seq2seq API를 사용한다.\n",
    "- **트랜스포머**(transformer) : RNN을 모두 제거하고 어텐션만 사용해 매우 좋은 성능을 내는 구조. GPT-2와 BERT 같은 모델의 기반이 된다.\n",
    "\n",
    "# 16.1 Char-RNN을 사용해 셰익스피어 문체 생성하기\n",
    "## 16.1.1 훈련 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "1122304/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# 셰익스피어 작품 다운로드\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자를 정수로 인코딩\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # 글자 수준 인코딩\n",
    "tokenizer.fit_on_texts(shakespeare_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 텍스트를 인코딩하여 각 글자를 ID로 나타냄\n",
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 # 1에서 39 대신 0에서 38까지 ID를 얻기 위해 1을 뺌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.2 순차 데이터셋을 나누는 방법\n",
    "훈련 세트, 검증 세트, 테스트 세트가 중복되지 않도록 만드는 것이 중요. 시계열을 다룰 때는 보통 시간에 따라 나눈다. <br/>\n",
    "암묵적으로 RNN은 시계열 데이터가 넓은 의미에서 **변하지 않는다**(stationary)고 가정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 처음 90%를 훈련 세트로 사용\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.3 순차 데이터를 윈도 여러 개로 자르기\n",
    "`window()` 메서드를 사용해 긴 시퀀스를 작은 많은 텍스트 윈도로 변환한다. <br/>\n",
    "- TBPTT(truncated backpropagation through time) : 부분 문자열 길이만큼만 역전파를 위해 펼쳐지는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100 # n_steps를 너무 짧게 하면 안 됨.\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True) # 모든 윈도가 동일한 글자를 포함하도록 지정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중첩 데이터셋(nested dataset) : 하나의 데이터셋으로 표현되는 윈도를 포함하는 데이터셋. 섞거나 배치를 만들 때 유용.\n",
    "- 플랫 데이터셋(flat dataset) : 데이터셋이 들어 있지 않는 데이터셋. 모델에 데이터셋이 아니라 텐서를 넣어야 하기 때문에 중첩 데이터셋 대신 사용.\n",
    "- ex) {{1, 2}, {3, 4, 5}} (중첩 데이터셋) ---(`flat_map` 메서드)---> {1, 2, 3, 4, 5} (플랫 데이터셋)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윈도를 배치로 만들고 입력과 타깃을 분리\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 벡터를 사용해 글자를 인코딩\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프리페칭 추가\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.4 Char-RNN 모델 만들고 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 9426/31370 [========>.....................] - ETA: 46:51 - loss: 1.8856"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.5 Char-RNN 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리를 위한 함수\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 텍스트의 다음 글자 예측\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.6 가짜 셰익스피어 텍스트를 생성하기\n",
    "생성된 텍스트의 다양성을 제어하려면 **온도**(temperature)라고 불리는 숫자로 로짓을 나눈다. 0에 가까울수록 높은 확률을 가진 글자를 택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 반복 호출하여 다음 글자를 얻고 텍스트에 추가하는 함수\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 온도를 다르게 해보며 테스트\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=1)) # 이 셰익스피어 모델은 1에 가까운 온도에서 잘 작동하는 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
