{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[참고](https://github.com/rickiepark/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 챗봇(chatbot) : 대화의 상대방이 자신을 사람이라고 생각하도록 속일 수 있는 기계.\n",
    "- 문자 단위 RNN(character RNN) : 문장에서 다음 글자를 예측하도록하는 훈련\n",
    "- **상태가 없는 RNN**(stateless RNN)을 사용하고 **상태가 있는 RNN**(stateful RNN)을 구축한다.\n",
    "- 텐서플로 애드온(Addon) 프로젝트에서 제공하는 seq2seq API를 사용한다.\n",
    "- **트랜스포머**(transformer) : RNN을 모두 제거하고 어텐션만 사용해 매우 좋은 성능을 내는 구조. GPT-2와 BERT 같은 모델의 기반이 된다.\n",
    "\n",
    "# 16.1 Char-RNN을 사용해 셰익스피어 문체 생성하기\n",
    "## 16.1.1 훈련 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "1122304/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# 셰익스피어 작품 다운로드\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자를 정수로 인코딩\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # 글자 수준 인코딩\n",
    "tokenizer.fit_on_texts(shakespeare_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 텍스트를 인코딩하여 각 글자를 ID로 나타냄\n",
    "import numpy as np\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 # 1에서 39 대신 0에서 38까지 ID를 얻기 위해 1을 뺌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.2 순차 데이터셋을 나누는 방법\n",
    "훈련 세트, 검증 세트, 테스트 세트가 중복되지 않도록 만드는 것이 중요. 시계열을 다룰 때는 보통 시간에 따라 나눈다. <br/>\n",
    "암묵적으로 RNN은 시계열 데이터가 넓은 의미에서 **변하지 않는다**(stationary)고 가정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 처음 90%를 훈련 세트로 사용\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.3 순차 데이터를 윈도 여러 개로 자르기\n",
    "`window()` 메서드를 사용해 긴 시퀀스를 작은 많은 텍스트 윈도로 변환한다. <br/>\n",
    "- TBPTT(truncated backpropagation through time) : 부분 문자열 길이만큼만 역전파를 위해 펼쳐지는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100 # n_steps를 너무 짧게 하면 안 됨.\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True) # 모든 윈도가 동일한 글자를 포함하도록 지정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중첩 데이터셋(nested dataset) : 하나의 데이터셋으로 표현되는 윈도를 포함하는 데이터셋. 섞거나 배치를 만들 때 유용.\n",
    "- 플랫 데이터셋(flat dataset) : 데이터셋이 들어 있지 않는 데이터셋. 모델에 데이터셋이 아니라 텐서를 넣어야 하기 때문에 중첩 데이터셋 대신 사용.\n",
    "- ex) {{1, 2}, {3, 4, 5}} (중첩 데이터셋) ---(`flat_map` 메서드)---> {1, 2, 3, 4, 5} (플랫 데이터셋)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윈도를 배치로 만들고 입력과 타깃을 분리\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 벡터를 사용해 글자를 인코딩\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프리페칭 추가\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.4 Char-RNN 모델 만들고 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "31370/31370 [==============================] - 12929s 412ms/step - loss: 1.7195\n",
      "Epoch 2/3\n",
      "31370/31370 [==============================] - 4020s 128ms/step - loss: 1.5448\n",
      "Epoch 3/3\n",
      "31370/31370 [==============================] - 4000s 128ms/step - loss: 1.5209\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=3) # 엄청 오래 걸려서 3번만 돌림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.5 Char-RNN 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리를 위한 함수\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떤 텍스트의 다음 글자 예측\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1.6 가짜 셰익스피어 텍스트를 생성하기\n",
    "생성된 텍스트의 다양성을 제어하려면 **온도**(temperature)라고 불리는 숫자로 로짓을 나눈다. 0에 가까울수록 높은 확률을 가진 글자를 택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 반복 호출하여 다음 글자를 얻고 텍스트에 추가하는 함수\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t the country have a suidor of the countrymen:\n",
      "what\n"
     ]
    }
   ],
   "source": [
    "# 온도를 다르게 해보며 테스트\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thrick on my, my\n",
      "high, thou art farcuus on toing yo\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1)) # 이 셰익스피어 모델은 1에 가까운 온도에서 잘 작동하는 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th \n",
      "reck padesic! ne'er fire-eat.\n",
      "yot hrapicag spok\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 좋은 텍스트를 생성하려면 GRU 층과 뉴런 수를 늘리고 더 오래 훈련하거나 규제를 추가해보자!\n",
    "\n",
    "## 16.1.7 상태가 있는 RNN\n",
    "상태가 있는 RNN : 한 훈련 배치를 처리한 후에 마지막 상태를 다음 훈련 배치의 초기 상태로 사용하는 RNN <br/>\n",
    "순차적이로 겹치지 않는 입력 시퀀스를 만들어야 하는데, `window()` 메서드에 `shift=n_steps`를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.repeat().batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상태가 있는 RNN을 만들 때 주의할 점\n",
    "1. 각 순환 층을 만들 때 `stateful=True`로 지정해야 함.\n",
    "2. 배치 크기를 알아야 하기 때문에 첫번째 층에 `batch_input_shape` 매개변수를 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, #recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2), #recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 함수를 사용해 에포크 끝마다 상태를 재설정\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 38s 114ms/step - loss: 2.9076\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 36s 117ms/step - loss: 2.2875\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 37s 117ms/step - loss: 2.1358\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 2.0491\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.9944\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 37s 117ms/step - loss: 1.9538\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.9246\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.8992\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.8817\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.8652\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 1.8542\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.8397\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.8291\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.8205\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.8139\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.8033\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.7982\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 1.7944\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7870\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.7829\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 1.7765\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.7727\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7691\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.7634\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.7595\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.7576\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.7518\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7527\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.7498\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 36s 116ms/step - loss: 1.7427\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7440\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.7386\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7372\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 37s 119ms/step - loss: 1.7350\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.7342\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 1.7298\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 1.7313\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 35s 110ms/step - loss: 1.7275\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 1.7247\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.7224\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7225\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7211\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.7201\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7167\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 1.7141\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.7134\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.7134\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.7120\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 1.7116\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 1.7065\n"
     ]
    }
   ],
   "source": [
    "# 모델을 컴파일하고 훈련\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.2 감정 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDb\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코딩\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\sarah\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f191ed11ee42d09e0bd305e7b1bfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d9d1312e964d1aaf9d742140514022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Size...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generating splits...'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generating train examples...'), FloatProgress(value=1.0, bar_style='info', layout=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Shuffling imdb_reviews-train.tfrecord...'), FloatProgress(value=0.0, max=25000.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generating test examples...'), FloatProgress(value=1.0, bar_style='info', layout=La…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Shuffling imdb_reviews-test.tfrecord...'), FloatProgress(value=0.0, max=25000.0), H…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generating unsupervised examples...'), FloatProgress(value=1.0, bar_style='info', l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Shuffling imdb_reviews-unsupervised.tfrecord...'), FloatProgress(value=0.0, max=500…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\sarah\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로 데이터셋을 사용해 IMDb 리뷰를 텍스트로 적재\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \") # 줄바꿈을 공백으로 바꿈\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 사전 구축\n",
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 많이 등장하는 단어 세 개 (???)\n",
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 많이 등장하는 단어 10,000개만 남기고 삭제\n",
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어를 ID로 바꾸는 전처리\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]], dtype=int64)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 몇 개에 대한 ID\n",
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 훈련 세트\n",
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 94s 109ms/step - loss: 0.5957 - accuracy: 0.6607\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 86s 111ms/step - loss: 0.3701 - accuracy: 0.8398\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 87s 111ms/step - loss: 0.2081 - accuracy: 0.9237\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 88s 113ms/step - loss: 0.1412 - accuracy: 0.9512\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 87s 112ms/step - loss: 0.1072 - accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "# 모델을 만들어 훈련\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.1 마스킹\n",
    "원래 데이터 그대로 모델이 패딩 토큰을 무시하도록 학습되어야하므로 `Embedding` 층을 만들 때 `mask_zero=True` 매개변수를 추가한다. <br/>\n",
    "구체적으로 `Embedding` 층이 `K.not_equal(inputs, 0)`과 같은 마스크 텐서(mask tensor)를 만든다. <br/>\n",
    "마스킹 층과 마스크 자동 전파는 `Squential` 모델에 가장 잘 맞고 복잡한 모델에서는 잘 작동하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 92s 109ms/step - loss: 0.6093 - accuracy: 0.6406\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 88s 113ms/step - loss: 0.3712 - accuracy: 0.8425\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 85s 109ms/step - loss: 0.1955 - accuracy: 0.9295\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 85s 109ms/step - loss: 0.1213 - accuracy: 0.9577\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 89s 113ms/step - loss: 0.1054 - accuracy: 0.9617\n"
     ]
    }
   ],
   "source": [
    "# 함수형 API를 사용하여 직접 마스킹 처리\n",
    "K = keras.backend\n",
    "embed_size = 128\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2.2 사전훈련된 임베딩 재사용하기\n",
    "모듈(module) : 모델에 추가하기 쉽게 텐서플로 허브(Hub) 프로젝트가 사전훈련시킨 모델 컴포넌트."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnlm-en-dim50 문장 임베딩 모듈 버전 1을 감성 분서거 모델에 사용\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", # 문장 인코더(sentence encoder) 다운로드\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 3s 3ms/step - loss: 0.5861 - accuracy: 0.6919\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 2s 3ms/step - loss: 0.5181 - accuracy: 0.7445\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 2s 3ms/step - loss: 0.5122 - accuracy: 0.7494\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 2s 3ms/step - loss: 0.5086 - accuracy: 0.7492\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 2s 3ms/step - loss: 0.5052 - accuracy: 0.7518\n"
     ]
    }
   ],
   "source": [
    "# IMDb 리뷰 데이터셋 다운로드\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.3 신경망 기계 번역을 위한 인코더-디코더 네트워크\n",
    "- 샘플링 소프트맥스(sampled softmax) : 타깃 단어에 대한 로짓과 타깃이 아닌 단어 중 무작위로 샘플링한 단어의 로짓만 고려하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.12.0-cp38-cp38-win_amd64.whl (639 kB)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.10.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.12.0 typeguard-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 인코더-디코더 모델\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True) # 최종 은닉 상태를 디코더로 보냄.\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler() # 각 스텝에서 디코더에게 이전 스텝의 출력이 무엇인지 알려주는 샘플러.\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512) # 은닉 상태 단기와 장기 두 개를 반환.\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)#, training=None)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3.1 양방향 RNN\n",
    "일반적인 순환 층은 인과적(casual)이다. 즉, 과거와 현재의 입력만 보고 출력을 생성한다. \n",
    "- 양방향 순환 층(bidirectional recurrent layer) : 하나는 왼쪽에서 오른쪽으로 단어를 읽고 다른 하나는 오른쪽에서 왼쪽으로 읽는 두 개의 순환 층."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_12 (GRU)                 (None, None, 10)          660       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 20)          1320      \n",
      "=================================================================\n",
      "Total params: 1,980\n",
      "Trainable params: 1,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 양방향 GRU 층\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
